{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46712c3d-3399-4a57-b25d-62361574d871",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier NBC\n",
    "\n",
    "### <span style='color:yellow'>The term naive of a NBC is attributed to the independence (naive) between the feature vector components of data. </span>\n",
    "\n",
    "### <span style='color:yellow'>The NBC is one of the simplest probabilistic classifier based on Bayes' therom.</span>\n",
    "\n",
    "### <span style='color:yellow'>In Bayes theorem, if we have two events A and A, then the probability of event A to be occured given B is given as follows:</span>\n",
    "\n",
    "$$\n",
    "    \\LARGE{P(A|B)} = {\\frac{P(B|A).P(A)}{P(B)}} \n",
    "$$\n",
    "\n",
    "### <span style='color:yellow'>P(A|B) is called posterior probability. </span>\n",
    "\n",
    "### <span style='color:yellow'>P(B|A) is termed as a class conditional probability: It can be optained from gaussian distribution. </span>\n",
    "\n",
    "\n",
    "### <span style='color:yellow'>P(A) is callled the prior of event A; practically we cont how many times event A is occured and use it as a prior. </span>\n",
    "\n",
    "### <span style='color:yellow'>P(A) is called the prior of event B; practically we cont how many time event A occured and use it as a prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5844150-2d0b-4c81-9a61-14ab8cbfdbd3",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier NBC for machine learning\n",
    "\n",
    "### <span style='color:yellow'> To integerate the NBC within other machine learning models, we should unify the mathmatical notations and  conventions, thus we will use teh following math,matical model: </span>  \n",
    "\n",
    "$$\n",
    "    \\LARGE{P(y|X)} = {\\frac{P(X|y).P(y)}{P(X)}} \n",
    "$$\n",
    "\n",
    "### <span style='color:yellow'> The above equation could be read as teh probability of class label y given the feature vector X ={x1,x2,x3,,,$x_n$}  (Remmember: the components of the feature vectors should be independent.) </span>  \n",
    "\n",
    "### <span style='color:yellow'> Remember: the feature vector components should be mutually independent.</span>  \n",
    "\n",
    "\n",
    "### <span style='color:yellow'>An example of feature commponents independence: the probability of detecting a hotel class (3* or 5*), based on the location and room sizes (location and room size are independent from each other).</span>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3a6af-74f8-4ce7-8eaa-2d9ecb86c736",
   "metadata": {},
   "source": [
    "# How the independence property affects NBC?\n",
    "\n",
    "\n",
    "### <span style='color:yellow'> At  real life application, the indepence among feature vector components might be challenging. </span>  \n",
    "\n",
    "\n",
    "### <span style='color:yellow'> We will consider that we have a dataset of N samples, wehere each sample comprises different and nutually independent feature vector components.</span>  \n",
    "\n",
    "\n",
    "### <span style='color:yellow'>With the independance assumption, we will be abel to factorize/ split $P(X|y)$ which is the main portion of the NBC, where we use the chain rule for mathmatical implementations: </span>  \n",
    "\n",
    "$$\n",
    "{P(X|y)=P(x_1|y)*P(x_2|y)*...*P(x_n|y)}\n",
    "$$\n",
    "\n",
    "### <span style='color:yellow'>Thus, the NBC fomula becoms:</span>\n",
    "\n",
    "$$\n",
    "P(y|X)= \\frac{P(x_1|y)*P(x_2|y)*...*P(x_n|y)P(y)}{P(X)}\n",
    "$$\n",
    "\n",
    "### <span style='color:yellow'> Beacuse the model is used for classification, we only care about the labels and the prior of labels, accordingly we neglect any part that does not contation y, i.e., we remove P(X) from denominator and keep only the nominator:</span>\n",
    "    \n",
    "$$\n",
    "P(y|X)= P(x_1|y)*P(x_2|y)*...*P(x_n|y)P(y)\n",
    "$$\n",
    "    \n",
    "### <span style='color:yellow'>Remember: P(y|X) is called the posterior probability, P(x|y) is called class conditional, P(y) is called the prior of labels (occurance or simply counts the labels of each class and express the number of each class as a prior).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a356c0-4a98-4853-9682-365b27e88344",
   "metadata": {},
   "source": [
    "# Classification/ class selection\n",
    "\n",
    "###  <span style='color:yellow'>To perform the classification, we want to predict the class label based on probabilistic value. Practically, the model predicts a vector of probabilities and the length of that vector is a function of how many unique class we have.</span>\n",
    "\n",
    "###  <span style='color:yellow'>Considering a feature vector of probabilities, we use argmax to obtain the index of the highest probability and retrieve the class label based on that index:</span>\n",
    "\n",
    "$$\n",
    "y=\\mathrm{argmax}_y=P(x_1|y)*P(x_2|y)*...*P(x_n|y)P(y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27968965-2d6b-468a-8e06-c11c8ef68bf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Log-trick \n",
    "\n",
    "### <span style='color:yellow'> Because we already factorized the class conditional probability P(X|y) into P(x_1|y)*P(x_2|y)*...*P(x_n|y) and the multiplication is utilized, we might be face overflow of multiplication.</span>\n",
    "\n",
    "\n",
    "### <span style='color:yellow'>Generally, the multiplication of several small numbers leads to a very small number, and that is simply the overflow problem in the multiplication.</span>\n",
    "\n",
    "\n",
    "### <span style='color:yellow'>To solve the overflow problem, we want to turn the multiplication into addition and that is done by using the logarithmic operators among the NBC model:</span>\n",
    "\n",
    "\n",
    "$$\n",
    "y=\\mathrm{argmax}_y=\\mathrm{log}(P(x_1|y))+\\mathrm{log}(P(x_2|y))+\\mathrm{log}...+\\mathrm{log}(P(x_n|y))\\mathrm{log}(P(y))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a7798-3e38-4e3f-9611-bdb38a1db382",
   "metadata": {},
   "source": [
    "# For practical implementations:\n",
    "\n",
    "### <span style='color:yellow'>The occurrence or frequency of each class label is used as a prior for prediction.</span>\n",
    "\n",
    "\n",
    "### <span style='color:yellow'>$P(x_i|y)$ is the class conditional probability and is estimated from the Gaussian distribution:</span>\n",
    "\n",
    "$$\n",
    "     P(x_i|y)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2_y}}.\\mathrm{exp}(-\\frac{(x_i-\\mu_y)^2}{2 \\sigma^2_y})\n",
    "$$\n",
    "\n",
    "### <span style='color:yellow'> The Gaussian distribution is illustrated at the following figure:</span>\n",
    "\n",
    "<img src='gdis.png' width=350>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff41d73f-dc3c-44f3-899c-00ed677e6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NaiveBayes:\n",
    "    # We do not need the constructor __init__\n",
    "    def fit(self,X,y):\n",
    "        # We need the prior P(y), and we need the mean and variance for teh class conditional and\n",
    "        n_samples,n_features=X.shape\n",
    "        self._classes=np.unique(y)\n",
    "        # The number of classes is important to obtain P(y)\n",
    "        n_classes=len(self._classes) \n",
    "        self.prior_y=np.zeros(n_classes)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012a97ec-40dd-4d95-b2c9-4f08be3b5637",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Initiate the mean and variance for each class'\n",
    "class NaiveBayes:\n",
    "    # We do not need the constructor __init__\n",
    "    def fit(self,X,y):\n",
    "        # We need the prior P(y), and we need the mean and variance for teh class conditional and\n",
    "        n_samples,n_features=X.shape\n",
    "        self._classes=np.unique(y)\n",
    "        # The number of classes is important to obtain P(y)\n",
    "        n_classes=len(self._classes) \n",
    "        \n",
    "        self.prior_y=np.zeros(n_classes,dtype=np.float64)\n",
    "        self._mean=np.zeros(n_classes,n_features)\n",
    "        self._var=np.zeros(n_classes,n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beabc5fb-c0b4-4d92-8420-27a720f0904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Compute the mean and variance for each class'\n",
    "class NaiveBayes:\n",
    "    # We do not need the constructor __init__\n",
    "    def fit(self,X,y):\n",
    "        # We need the prior P(y), and we need the mean and variance for teh class conditional and\n",
    "        n_samples,n_features=X.shape\n",
    "        self._classes=np.unique(y)\n",
    "        # The number of classes is important to obtain P(y)\n",
    "        n_classes=len(self._classes) \n",
    "        \n",
    "        self.prior_y=np.zeros(n_classes,dtype=np.float64)\n",
    "        self._mean=np.zeros(n_classes,n_features)\n",
    "        self._var=np.zeros(n_classes,n_features)\n",
    "        \n",
    "        for class_ in self._classes:\n",
    "            # Retriving class of data based on masking the index of the class and the labels y\n",
    "            X_class=X[class_==y]\n",
    "            # Separate mean for each class\n",
    "            self._mean[c,:]=X_class.mean(axis=0)\n",
    "            # Separate variance for each class\n",
    "            self._var[c,:]=X_class.var(zxis=0)\n",
    "            # Separate prior fo reach class\n",
    "            self.prior_y[c]=X_class.shape[0]/float(n_samples)\n",
    "        # To predict method:\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f555f4-5eaf-450e-9458-d65729b06c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Building predict method'\n",
    "class NaiveBayes:\n",
    "    # We do not need the constructor __init__\n",
    "    def fit(self,X,y):\n",
    "        # We need the prior P(y), and we need the mean and variance for teh class conditional and\n",
    "        n_samples,n_features=X.shape\n",
    "        self._classes=np.unique(y)\n",
    "        # The number of classes is important to obtain P(y)\n",
    "        n_classes=len(self._classes) \n",
    "        \n",
    "        self.prior_y=np.zeros(n_classes,dtype=np.float64)\n",
    "        self._mean=np.zeros((n_classes,n_features),dtype=np.float64)\n",
    "        self._var=np.zeros((n_classes,n_features),dtype=np.float64)\n",
    "        \n",
    "        for class_ in self._classes:\n",
    "            # Retriving class of data based on masking the index of the class and the labels y\n",
    "            X_class=X[class_==y]\n",
    "            # Separate mean for each class\n",
    "            self._mean[class_,:]=X_class.mean(axis=0)\n",
    "            # Separate variance for each class\n",
    "            self._var[class_,:]=X_class.var(axis=0)\n",
    "            # Separate prior fo reach class\n",
    "            self.prior_y[class_]=X_class.shape[0]/float(n_samples)\n",
    "        # To predict method:\n",
    "            \n",
    "    def predict(self,X):\n",
    "        y_predicted=[self._predict(x) for x in X]\n",
    "        return y_predicted\n",
    "    \n",
    "    def _predict(self,x):\n",
    "        posteriors_prob=[] #  Here we store the prediction\n",
    "        # Iterate over classes to  computes its log prior anc class conditional probability\n",
    "        for idx,class_ in enumerate(self._classes):\n",
    "            # Computing the log of the class\n",
    "            prior=np.log(self.prior_y[idx])\n",
    "            # Computing the Gaussian distribution\n",
    "            class_conditonal_prob= np.sum(np.log(self._Gaussian_dist(idx,x))) \n",
    "            posterior=prior+class_conditonal_prob\n",
    "            posteriors_prob.append(posterior) #5\n",
    "        return(self._classes[np.argmax(posteriors_prob)])\n",
    "        \n",
    "    def _Gaussian_dist(self,idx,x):  #3\n",
    "        # Class_idx will be inserted when we call this private method\n",
    "        # Retrieving the mean for each class based on its index\n",
    "        mean=self._mean[idx]\n",
    "        # Retrieving the variance for each class based on its index\n",
    "        var=self._var[idx]\n",
    "        # The numerator for the Gaussian distribution\n",
    "        numerator=np.exp(-(x-mean)**2 / (2*var))\n",
    "        # The denominator for the Gaussian distribution\n",
    "        denominator=np.sqrt(2*np.pi*var)\n",
    "        pdf=numerator/denominator\n",
    "        return pdf\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a74aa03-e927-4ef8-99c4-76cdb74b243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Let us test the NaiveBase classifier on a cllassification dataset'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7bfc30-6040-4b8f-90f9-b33d6f0299ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=datasets.make_classification(n_samples=1000, n_features=10,n_classes=2,random_state=1234)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,shuffle=True,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7562bf52-fdaf-4c30-9582-271334fa7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "fig,ax=plt.subplots(1,1)\n",
    "ax.scatter(X[:,5],X[:,6],c=y,cmap='viridis',marker='o',s=20)\n",
    "ax.axis('square')\n",
    "ax.axes.get_xaxis().set_ticks([])\n",
    "ax.axes.get_yaxis().set_ticks([])\n",
    "ax.set_xlabel('Feature 1',size=14,weight='bold')\n",
    "ax.set_ylabel('Feature2',size=14,weight='bold')\n",
    "plt.show(block=False)\n",
    "plt.pause(5)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da71e26d-ae90-400b-99f9-e829a2be1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04961880-c4bf-413e-9258-6f7a41b400d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)\n",
    "prediction=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff45d60b-f5f0-4a46-87ec-f774879434f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(list(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b731e33-6519-429b-a340-96f5e501a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Let us define the accurancy metrics'\n",
    "def prediction_accuracy(y_true,y_predicted):\n",
    "    return (np.sum(y_true==y_predicted))/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "600ecacc-4cef-431d-a2bf-c4f35e9fdd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy= prediction_accuracy(y_test,prediction)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc822d-8db3-486a-8542-b5b43362c0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
